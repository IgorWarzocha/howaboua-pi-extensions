{
  "id": "4e1a1671",
  "title": "[Phase 2] Read-assist: Streaming for large files",
  "tags": [
    "pi-hash",
    "phase-2",
    "read-tool",
    "streaming"
  ],
  "status": "open",
  "created_at": "2026-02-12T21:37:18.344Z",
  "assigned_to_session": "5d994b6f-5c75-4f28-8a94-ed305756b8c1"
}

## Goal
**Read tool assisting hash-patch** - Support large files with streaming and chunked output.

## Reference
oh-my-pi has `streamHashLinesFromUtf8()` generator (lines 330-420 in hashline.ts).

## Current State
pi-hash read tool loads entire file into memory then truncates output. For very large files:
1. Entire file loaded into memory before truncation
2. Memory spike for large files

## What's Needed

True streaming would:
1. Read file line by line
2. Format each line with hash
3. Yield chunks as they're processed
4. Never load entire file into memory

## Implementation

```typescript
async function* streamHashLines(
  filePath: string,
  options: { startLine?: number; maxLines?: number; maxBytes?: number }
): AsyncGenerator<string> {
  const stream = fs.createReadStream(filePath, { encoding: 'utf8' });
  const reader = readline.createInterface({ input: stream });
  
  let lineNo = options.startLine ?? 1;
  let bytes = 0;
  
  for await (const line of reader) {
    const formatted = `${lineNo}:${computeLineHash(line)}|${line}`;
    bytes += formatted.length;
    
    if (options.maxBytes && bytes > options.maxBytes) break;
    if (options.maxLines && lineNo > options.maxLines) break;
    
    yield formatted;
    lineNo++;
  }
}
```

## Status
**Not implemented** - Deferred as current truncation approach handles typical use cases.

## Acceptance Criteria
- [ ] Large files (>1000 lines) handled without loading entire file into memory
- [ ] Output streamed/chunked
- [ ] Memory usage constant regardless of file size

## Files
- `pi-extensions-dev/pi-hash/src/read/executor.ts`
